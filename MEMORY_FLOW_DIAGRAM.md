# ğŸ§  Memory Flow Diagram - Complete System Architecture

## ğŸ“Š System Overview

```
LM Studio (User Interface)
    â†“
MCP Server (v6_efc)
    â†“
Backend API (FastAPI port 8000)
    â†“
Symbiosis Router V4 (Orchestrator)
    â†“
9-Layer Optimal Memory System
    â†“
Databases (Qdrant + Neo4j)
```

---

## ğŸ¯ Complete Data Flow (Step-by-Step)

### 1ï¸âƒ£ USER INPUT â†’ MCP SERVER
```
User types in LM Studio chat
    â†“
LM Studio Config: lm-studio-config-v6-efc.json
    â”œâ”€ Points to: mcp/symbiosis_mcp_server_v6_efc.py
    â”œâ”€ Environment: SYMBIOSIS_API_URL=http://localhost:8000
    â””â”€ Tools: efc_record_feedback (Layer 9 learning)
    â†“
MCP Server Starts
    Location: mcp/symbiosis_mcp_server_v6_efc.py
    Port: stdio (MCP protocol)
    Function: Bridge LM Studio â†” Backend
```

### 2ï¸âƒ£ MCP â†’ BACKEND API
```
MCP sends HTTP POST request
    â†“
URL: http://localhost:8000/chat/turn
    â†“
Payload:
    {
        "user_message": "Hei",
        "assistant_draft": "Hei! Hvordan kan jeg hjelpe deg?",
        "session_id": "lm_studio_session_123"
    }
    â†“
Backend Entry Point: apis/unified_api/main.py
    â”œâ”€ FastAPI app initialization
    â”œâ”€ Imports all routers
    â””â”€ Runs on port 8000
```

### 3ï¸âƒ£ BACKEND ROUTING
```
main.py receives request
    â†“
Routes to: apis/unified_api/routers/chat.py
    â†“
Endpoint: POST /chat/turn
    â†“
Function: handle_turn()
    â”œâ”€ Validates input
    â”œâ”€ Extracts user_message, assistant_draft, session_id
    â””â”€ Calls: symbiosis_router_v4.handle_chat_turn()
```

### 4ï¸âƒ£ SYMBIOSIS ROUTER V4 (ORCHESTRATOR)
```
Location: tools/symbiosis_router_v4.py
Function: Master orchestrator for 9-layer memory system

Flow:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ handle_chat_turn()                  â”‚
    â”‚  - user_message: "Hei"              â”‚
    â”‚  - assistant_draft: "Hei! ..."      â”‚
    â”‚  - session_id: "lm_studio_..."      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
    Runtime Sequence (per chat turn):
    1. Layer 4 (DDE) â†’ Analyze input & decide which layers to activate
    2. Read layers (1-3) â†’ Fetch relevant memory (on demand)
    3. Layer 5 (AME) â†’ Fact enforcement & contradiction check
    4. Layer 6 (MLC) â†’ Pattern learning (if applicable)
    5. Maintenance layers (7-9) â†’ Runs on writes/updates, not every turn
```

---

## ğŸ—ï¸ 9-LAYER OPTIMAL MEMORY SYSTEM

### **Runtime Flow (per chat turn):**
```
1. Layer 4 (DDE) â†’ Routing & decision
2. Layers 1-3 â†’ Read memory (on-demand)
3. Layer 5 (AME) â†’ Fact enforcement
4. Layer 6 (MLC) â†’ Pattern learning (if applicable)
5. Layers 7-9 â†’ Maintenance (write-time only, not per turn)
```

---

### **Layer 4: Dynamic Decision Engine (DDE)** ğŸ¯ RUNS FIRST
```
File: tools/dynamic_decision_engine.py
Purpose: Routing logic (which layers to activate)
When: Beginning of every chat turn

Methods:
    - analyze_input(user_message, assistant_draft)
    - decide_layers(analysis_result)
    - get_routing_decision(user_message)

Logic:
    IF query contains identity words â†’ activate Layer 5 (AME)
    IF query is philosophical â†’ activate Layer 3 (Neo4j)
    IF query needs context â†’ activate Layer 2 (SMM)
    ALWAYS activate Layer 1 (CMC) for fact checking

Output:
    routing_decision = {
        "activated_layers": [1, 3, 5],
        "decision_reason": "identity + EFC-domain",
        "contradiction_check": true
    }
```

---

## ğŸ“– READ LAYERS (On-Demand Memory Retrieval)

### **Layer 1: Canonical Memory Core (CMC)**
```
File: tools/canonical_memory_core.py
Purpose: Ground truth facts (identity, family, preferences)

Methods:
    - set_fact(domain, key, value, confidence)
    - get_fact(domain, key)
    - get_facts_for_domain(domain)
    - query_related_facts(query_text)

Storage:
    Qdrant collection: "canonical_facts"
    Metadata: {domain, key, value, confidence, timestamp}

Example Facts:
    domain="identity" key="name" value="Brukeren heter Morten"
    domain="family" key="children" value="3 barn: Joakim, Isak Andreas, Susanna"
```

### **Layer 2: Semantic Mesh Memory (SMM)**
```
File: tools/semantic_mesh_memory.py
Purpose: Conversational context + semantic patterns

Methods:
    - store_turn(user_msg, assistant_msg, session_id, metadata)
    - retrieve_similar_conversations(query, limit, threshold)
    - get_session_history(session_id, limit)

Storage:
    Qdrant collection: "semantic_mesh"
    Metadata: {session_id, role, timestamp, turn_number}

Use Case:
    "Remember conversation about EFC theory from yesterday"
```

### **Layer 3: Neo4j Graph Memory**
```
File: tools/neo4j_graph_memory.py
Purpose: Knowledge graph (concepts, relationships, ontology)

Methods:
    - store_concept(name, description, properties)
    - link_concepts(from_concept, to_concept, relation_type)
    - query_graph(cypher_query)
    - find_related_concepts(concept_name, max_depth)

Storage:
    Neo4j database
    Nodes: Concept, Document, Chunk
    Relationships: SUPPORTS, CONSTRAINS, PART_OF, DERIVES_FROM

Example:
    (Entropy)-[:SUPPORTS]->(Second Law)
    (EFC Theory)-[:DERIVES_FROM]->(Cosmology)
```

---

## âœ… ENFORCEMENT LAYER

### **Layer 5: Adaptive Memory Enforcer (AME)** â­ CRITICAL
```
File: tools/adaptive_memory_enforcer.py
Purpose: Intelligent fact correction with LLM-based contradiction detection

Methods:
    - enforce_memory(user_msg, assistant_draft, session_id)
        â†“
    - _should_check_facts(user_msg, assistant_draft)
        â”œâ”€ Identity questions? â†’ YES
        â”œâ”€ Factual claims? â†’ YES
        â””â”€ Casual chat? â†’ NO (skip)
        â†“
    - _retrieve_canonical_facts(query_text)
        â””â”€ Searches Layer 1 (CMC) for relevant facts
        â†“
    - _decide(assistant_draft, canonical_facts)
        â”œâ”€ Calls _contradicts() for each fact
        â”œâ”€ IF contradiction â†’ OVERRIDE path
        â””â”€ IF no contradiction â†’ TRUST_LLM path
        â†“
    - _contradicts(draft, canonical_fact) â­ LLM-BASED
        â”œâ”€ Sends prompt to local LM Studio
        â”œâ”€ URL: http://localhost:1234/v1/chat/completions
        â”œâ”€ Model: <local-model>
        â”œâ”€ Prompt: "Does this draft contradict this fact?"
        â”œâ”€ Returns: {contradicts: true/false, explanation: "..."}
        â””â”€ Fallback: Pattern matching (numbers, names, negations)

Decision Paths:
    1. OVERRIDE:
        - Contradiction detected by LLM
        - Return canonical fact verbatim
        - Example: "Jeg vet ikke" â†’ "Brukeren heter Morten"
    
    2. TRUST_LLM:
        - No contradiction detected
        - Use LLM's draft as-is
        - NO augmentation (removed AUGMENT path completely!)

Example Flow:
    User: "Hva heter jeg?"
    Draft: "Jeg vet ikke"
    Canonical: "Brukeren heter Morten"
    LLM Check: "YES, these contradict"
    Decision: OVERRIDE
    Output: "Brukeren heter Morten"

Why LLM-Based?
    - Understands semantic contradiction (not just keyword matching)
    - Detects: numbers, names, negations, implicit contradictions
    - Fallback: Pattern matching if LLM fails
```

---

## ğŸ§  META LAYER (Background/Periodic)

### **Layer 6: Meta-Learning Context (MLC)**
```
File: tools/meta_learning_context.py
Purpose: Cross-domain pattern learning (EFC Layer 9)
When: Activated when sufficient signal exists for pattern detection (not every turn)

Methods:
    - record_pattern(pattern_type, context, domains)
    - detect_universal_patterns(threshold=3)
    - augment_response(response, detected_patterns)

Storage:
    JSON: meta_patterns.json
    Qdrant collection: "meta_patterns"

Example:
    Pattern: "Conservation principles apply universally"
    Domains: ["physics", "information_theory", "biology"]
    Status: Universal (â‰¥3 domains)

Note: This is a meta-layer that runs periodically, not inline with every chat turn.
```

---

## ğŸ”§ MAINTENANCE LAYERS (Write-Time Only)

**Important**: Layers 7-9 run when facts are written/updated, NOT on every chat turn.
They maintain memory integrity, confidence, and causal relationships.

### **Layer 7: Memory Integrity Regulator (MIR)**
```
File: tools/memory_integrity_regulator.py
Purpose: Conflict detection + resolution
When: Triggered when new facts are added or updated

Methods:
    - check_conflicts(new_fact, existing_facts)
    - resolve_conflict(fact_a, fact_b)
    - audit_memory_consistency()

Logic:
    IF fact_new.value != fact_existing.value:
        IF fact_new.confidence > fact_existing.confidence:
            UPDATE fact
        ELSE:
            FLAG conflict for human review
```

### **Layer 8: Memory Confidence Adjuster (MCA)**
```
File: tools/memory_confidence_adjuster.py
Purpose: Dynamic confidence scoring
When: Runs on fact usage, updates, and periodic aging

Methods:
    - adjust_confidence(fact, feedback)
    - decay_old_facts(time_threshold)
    - boost_frequently_used(fact)

Example:
    Fact: "User prefers dark mode"
    Initial confidence: 0.8
    After 5 successful uses: 0.95
    After 30 days unused: 0.7 (decay)
```

### **Layer 9: Memory Causality Engine (MCE)**
```
File: tools/memory_causality_engine.py
Purpose: Track fact dependencies + causal chains
When: Triggered when facts with dependencies are modified or deleted

Methods:
    - link_causality(cause_fact, effect_fact)
    - trace_causal_chain(fact_id)
    - invalidate_dependent_facts(root_fact_id)

Example:
    IF fact="User lives in Oslo" is deleted
    THEN invalidate dependent facts:
        - "User's timezone is CET"
        - "User speaks Norwegian"
```

---

## ğŸ”„ Complete Request/Response Flow

### Example 1: Simple Greeting (No Memory Needed)
```
1. User: "Hei"
2. LM Studio â†’ MCP v6 â†’ Backend (8000) â†’ Router v4
3. Router v4 calls:
   â”œâ”€ Layer 4 (DDE): "No fact checking needed"
   â””â”€ Layer 5 (AME): _should_check_facts() â†’ FALSE
4. Router returns: assistant_draft unchanged
5. Backend â†’ MCP â†’ LM Studio
6. Output: "Hei! Hvordan kan jeg hjelpe deg?"
```

### Example 2: Identity Question (Memory Override)
```
1. User: "Hva heter jeg?"
2. LM Studio â†’ MCP v6 â†’ Backend (8000) â†’ Router v4
3. Router v4 calls:
   â”œâ”€ Layer 4 (DDE): "Identity query detected"
   â”œâ”€ Layer 5 (AME): _should_check_facts() â†’ TRUE
   â”‚   â”œâ”€ _retrieve_canonical_facts("identity name")
   â”‚   â”‚   â””â”€ Layer 1 (CMC): Returns "Brukeren heter Morten"
   â”‚   â”œâ”€ _decide(draft="Jeg vet ikke", facts=["Morten"])
   â”‚   â”‚   â”œâ”€ _contradicts() calls LLM at localhost:1234
   â”‚   â”‚   â”‚   Prompt: "Draft: 'Jeg vet ikke' vs Fact: 'Morten'"
   â”‚   â”‚   â”‚   LLM: {contradicts: true}
   â”‚   â”‚   â””â”€ Decision: OVERRIDE
   â”‚   â””â”€ Returns: "Brukeren heter Morten"
   â””â”€ Layer 7 (MIR): Validates no conflicts
4. Backend â†’ MCP â†’ LM Studio
5. Output: "Brukeren heter Morten"
```

### Example 3: EFC Theory Question (Graph + Learning)
```
1. User: "Hva er entropiprinsippet i EFC?"
2. LM Studio â†’ MCP v6 â†’ Backend (8000) â†’ Router v4
3. Router v4 calls:
   â”œâ”€ Layer 4 (DDE): "EFC theory query detected"
   â”œâ”€ Layer 3 (Neo4j): Query for (Entropy)-[:PART_OF]->(EFC)
   â”‚   â””â”€ Returns: Graph structure + related concepts
   â”œâ”€ Layer 2 (SMM): Retrieve similar past conversations
   â”‚   â””â”€ Returns: Previous entropy discussions
   â”œâ”€ Layer 6 (MLC): Check for universal patterns
   â”‚   â””â”€ Detects: "Conservation principles" pattern
   â””â”€ Layer 5 (AME): No identity facts â†’ TRUST_LLM
4. Backend â†’ MCP â†’ LM Studio
5. Output: "I EFC representerer entropi..." (enriched with context)
6. MCP v6 tool: efc_record_feedback() â†’ stores pattern in Layer 6
```

---

## ğŸ“ File Structure Reference

### Core Memory Modules (i rekkefÃ¸lge)

**ğŸ¯ Orchestrator:**
1. [`tools/symbiosis_router_v4.py`](tools/symbiosis_router_v4.py) - Master orchestrator (kaller alle 9 lag)

**ğŸ§  9-Layer Memory System:**
2. [`tools/canonical_memory_core.py`](tools/canonical_memory_core.py) - **Layer 1**: Ground truth facts
3. [`tools/semantic_mesh_memory.py`](tools/semantic_mesh_memory.py) - **Layer 2**: Conversational context
4. [`tools/neo4j_graph_memory.py`](tools/neo4j_graph_memory.py) - **Layer 3**: Knowledge graph
5. [`tools/dynamic_decision_engine.py`](tools/dynamic_decision_engine.py) - **Layer 4**: Routing logic
6. [`tools/adaptive_memory_enforcer.py`](tools/adaptive_memory_enforcer.py) - **Layer 5**: Fact enforcement â­ CRITICAL
7. [`tools/meta_learning_context.py`](tools/meta_learning_context.py) - **Layer 6**: EFC pattern learning
8. [`tools/memory_integrity_regulator.py`](tools/memory_integrity_regulator.py) - **Layer 7**: Conflict resolution
9. [`tools/memory_confidence_adjuster.py`](tools/memory_confidence_adjuster.py) - **Layer 8**: Confidence scoring
10. [`tools/memory_causality_engine.py`](tools/memory_causality_engine.py) - **Layer 9**: Causal dependencies

### MCP Servers (LM Studio Integration)

**ğŸŸ¢ ACTIVE:**
- [`mcp/symbiosis_mcp_server_v6_efc.py`](mcp/symbiosis_mcp_server_v6_efc.py) - Full EFC + 9 layers (PRODUCTION)

**ğŸ“¦ Alternative:**
- [`mcp/symbiosis_mcp_server_v5_minimal.py`](mcp/symbiosis_mcp_server_v5_minimal.py) - Simple version (1 tool only)

**ğŸ—„ï¸ ARCHIVED:**
- [`mcp/symbiosis_mcp_server.py`](mcp/symbiosis_mcp_server.py) - v3 (old)
- [`mcp/symbiosis_mcp_server_v2.py`](mcp/symbiosis_mcp_server_v2.py) - v2
- [`mcp/symbiosis_mcp_server_v4.py`](mcp/symbiosis_mcp_server_v4.py) - v4

### Backend API (FastAPI port 8000)

**Entry Point:**
- [`apis/unified_api/main.py`](apis/unified_api/main.py) - FastAPI app initialization

**Routers:**
- [`apis/unified_api/routers/chat.py`](apis/unified_api/routers/chat.py) - POST /chat/turn endpoint
- [`apis/unified_api/routers/efc_meta_learning.py`](apis/unified_api/routers/efc_meta_learning.py) - EFC learning endpoints
- [`apis/unified_api/routers/msty_context.py`](apis/unified_api/routers/msty_context.py) - Context management

### Configuration Files

**LM Studio MCP Config:**
- [`lm-studio-config-v6-efc.json`](lm-studio-config-v6-efc.json) - ğŸŸ¢ ACTIVE: Full system
- [`lm-studio-config-v5.json`](lm-studio-config-v5.json) - Minimal version

**Environment & Documentation:**
- `.env` - Database credentials (NEVER commit!)
- [`VERSION_CONTROL.md`](VERSION_CONTROL.md) - Complete version documentation
- [`MEMORY_FLOW_DIAGRAM.md`](MEMORY_FLOW_DIAGRAM.md) - This file (you are here)

### Supporting Tools & Scripts

**Ingestion Pipeline:**
- [`tools/orchestrator_v2.py`](tools/orchestrator_v2.py) - Data ingestion orchestrator
- [`tools/authority_filter.py`](tools/authority_filter.py) - Authoritative source validation
- [`tools/batch_ingest.py`](tools/batch_ingest.py) - Batch document processing

**EFC Pattern Learning:**
- [`tools/efc_pattern_learner.py`](tools/efc_pattern_learner.py) - Cross-domain pattern detection
- [`tools/chat_intention_bridge.py`](tools/chat_intention_bridge.py) - Chat â†’ EFC learning bridge

**Testing & Monitoring:**
- [`test_theory_auth.py`](test_theory_auth.py) - Test authority filter
- [`test_backend_chat.py`](test_backend_chat.py) - Test backend chat API
- [`test_memory_system.py`](test_memory_system.py) - Test memory layers
- [`cleanup_test_databases.py`](cleanup_test_databases.py) - Clean test collections
- [`monitor_ingest.sh`](monitor_ingest.sh) - Monitor ingestion progress
- [`monitor_augmentation.sh`](monitor_augmentation.sh) - Monitor augmentation logs

---

## ğŸ”§ System Dependencies

### Databases
```
Qdrant (Vector Database)
â”œâ”€â”€ URL: From .env QDRANT_URL
â”œâ”€â”€ Collections:
â”‚   â”œâ”€â”€ efc (9,588 vectors) - Production EFC data
â”‚   â”œâ”€â”€ canonical_facts (0 vectors) - Layer 1 storage
â”‚   â””â”€â”€ semantic_mesh (0 vectors) - Layer 2 storage
â””â”€â”€ API Key: From .env QDRANT_API_KEY

Neo4j (Graph Database)
â”œâ”€â”€ URL: From .env NEO4J_URI
â”œâ”€â”€ Nodes: 13,648 production nodes
â”œâ”€â”€ Labels: Concept, Document, Chunk
â”œâ”€â”€ Relationships: SUPPORTS, CONSTRAINS, PART_OF
â””â”€â”€ Credentials: .env NEO4J_USER / NEO4J_PASSWORD
```

### LLM Services
```
Local LM Studio
â”œâ”€â”€ API: http://localhost:1234/v1/chat/completions
â”œâ”€â”€ Used by: adaptive_memory_enforcer.py (_contradicts method)
â”œâ”€â”€ Purpose: Semantic contradiction detection
â””â”€â”€ Model: User's selected model (e.g., llama-3.3-70b-instruct)

OpenAI Client
â”œâ”€â”€ Configured in: adaptive_memory_enforcer.py
â”œâ”€â”€ Base URL: http://localhost:1234/v1
â”œâ”€â”€ API Key: "lm-studio" (placeholder)
â””â”€â”€ Fallback: Pattern matching if LLM fails
```

---

## ğŸš€ Startup Sequence

### 1. Start Databases (Already Running)
```bash
# Qdrant - cloud hosted (always on)
# Neo4j - check status:
curl http://localhost:7474/browser/

# If Neo4j down:
neo4j start
```

### 2. Start Backend API
```bash
cd /Users/morpheus/energyflow-cosmology
source .venv/bin/activate
cd apis/unified_api
python -m uvicorn main:app --port 8000 --reload &

# Verify:
curl http://localhost:8000/docs
```

### 3. Configure LM Studio MCP
```
1. Open LM Studio
2. Go to: Developer â†’ MCP Settings
3. Load config: lm-studio-config-v6-efc.json
4. Verify SYMBIOSIS_API_URL=http://localhost:8000
5. Restart LM Studio (critical!)
```

### 4. Verify MCP Connection
```bash
# Check MCP server logs in LM Studio
# Should see: "MCP server v6_efc started successfully"
# Should see: Tool registered: efc_record_feedback
```

### 5. Test Complete Flow
```
In LM Studio chat:
1. "Hei" â†’ Should return greeting (no augmentation)
2. "Hva heter jeg?" â†’ Should override with canonical fact
3. "Hva er EFC?" â†’ Should query graph + learning patterns
```

---

## ğŸ› Troubleshooting Flow

### Issue: "Hei" returns identity facts
```
Problem: AUGMENT logic still active
Solution: Already fixed - adaptive_memory_enforcer.py AUGMENT removed
Verify: Check line 220-325 in adaptive_memory_enforcer.py
```

### Issue: Backend not responding
```
Problem: Wrong startup command
Fix:
    âŒ uvicorn tools.symbiosis_router_v4:app
    âœ… cd apis/unified_api && uvicorn main:app --port 8000
```

### Issue: MCP not connecting
```
Problem: LM Studio not restarted after config change
Fix:
    1. Quit LM Studio completely
    2. Restart LM Studio
    3. Check Developer â†’ MCP Settings
    4. Verify v6_efc config loaded
```

### Issue: Contradiction detection failing
```
Problem: LM Studio not running on port 1234
Fix:
    1. Start LM Studio
    2. Load a model
    3. Check localhost:1234/v1/models
    4. Fallback: Pattern matching will activate
```

---

## ğŸ“Š Memory System Statistics

### Current State (After Cleanup)
```
Qdrant:
â”œâ”€â”€ efc: 9,588 vectors (production)
â”œâ”€â”€ canonical_facts: 0 vectors (clean)
â”œâ”€â”€ semantic_mesh: 0 vectors (clean)
â””â”€â”€ private: 0 vectors (deleted)

Neo4j:
â”œâ”€â”€ Production nodes: 13,648
â”œâ”€â”€ Private nodes: 0 (deleted)
â”œâ”€â”€ Concepts: ~5,000
â”œâ”€â”€ Documents: ~1,500
â””â”€â”€ Relationships: ~8,000

Layer Status:
âœ… All 9 layers operational
âœ… No AUGMENT pollution
âœ… LLM contradiction detection active
âœ… Pattern matching fallback ready
```

---

## ğŸ¯ Next Steps

### Pending Implementation
1. **Multi-Fact Synthesis** (Layer 5 enhancement)
   - Query: "Hvem er barna mine?"
   - Current: Returns first fact only
   - Needed: Merge related facts intelligently

2. **EFC Pattern Learning** (Layer 6 testing)
   - Use efc_record_feedback tool in LM Studio
   - Test cross-domain pattern detection
   - Verify universal patterns (â‰¥3 domains)

3. **Production Deployment**
   - Create .env.example template
   - Document deployment checklist
   - Test with fresh Python environment

---

## ğŸ“š Reference Documentation

- **System Overview**: START-HERE.md
- **Version Control**: VERSION_CONTROL.md
- **Ingestion Pipeline**: tools/INGESTION_PIPELINE.md
- **API Documentation**: api/README_API.md
- **EFC Theory**: theory/formal/efc_master.pdf
- **Meta Layer**: meta/README.md

---

**Last Updated**: 2025-12-12  
**System Status**: ğŸŸ¢ Operational - Ready for production testing  
**Private Data**: âœ… Fully cleaned (0 vectors, 0 nodes)
